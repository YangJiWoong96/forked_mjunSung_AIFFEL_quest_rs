{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "929a0cd6",
   "metadata": {},
   "source": [
    "# 프로젝트 : 커스텀 프로젝트 직접 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3bc182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy\n",
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04379701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dfd371ddf44c7293be4ef77224940e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc233fda2914c2dba8141e27a7f45e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset nsmc/default (download: 18.62 MiB, generated: 20.90 MiB, post-processed: Unknown size, total: 39.52 MiB) to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d301b882424943a7401dd0f38e88c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4dafc688404294bc7bde2be8267335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c0576ae0874611b926dc26cc3d5c90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.89M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ce13652a9b4378949eb23cd3cfcffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsmc downloaded and prepared to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5ec777d3de54f6f95d33c0e725fb934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 150000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"nsmc\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee2fcc",
   "metadata": {},
   "source": [
    "### model, tokenizer load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5925b95a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef6785a5518c47f0b42c0f2ceb370923",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/425 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676e36f838404496b97e699413dc2e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/424M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541e08b8a7ae4324b21b17e94f061233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/289 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2a2e8aaea764677b601d66d0c76e420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/243k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea956b29c48946f6860a20593129cbad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/483k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c937d84436944869915955b3eeb98bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "\n",
    "model_name = \"klue/bert-base\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels = 2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9de1f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=768, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab32e8",
   "metadata": {},
   "source": [
    "skt-kobert가 해당 데이터셋에 대해서 90프로 넘는 accuracy를 달성했는데 dropout을 헤드부분에서 적용하여 참고하여 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fdae062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomClassificationHead(\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 기존 분류 헤드 교체 (드롭아웃 추가)\n",
    "class CustomClassificationHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout_rate=0.5):\n",
    "        super(CustomClassificationHead, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.out_proj = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, features):\n",
    "        x = self.dropout(features)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# 새로운 분류 헤드 설정\n",
    "model.classifier = CustomClassificationHead(\n",
    "    input_dim=model.config.hidden_size,\n",
    "    output_dim=2,  # 클래스 수\n",
    "    dropout_rate=0.5\n",
    ")\n",
    "\n",
    "# 확인\n",
    "print(model.classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "422a3c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        max_length = 128,\n",
    "        return_token_type_ids = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46a022e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eede2ea5409452f89a3c9a77cf430fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d87cc25c056456f83ad8eb9ddc5987e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train\n",
    "train_dataset = dataset[\"train\"].map(transform, batched=True)\n",
    "\n",
    "# val\n",
    "val_dataset = dataset[\"test\"].map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "881d0170",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [2,\n",
       "  1376,\n",
       "  831,\n",
       "  2604,\n",
       "  18,\n",
       "  18,\n",
       "  4229,\n",
       "  9801,\n",
       "  2075,\n",
       "  2203,\n",
       "  2182,\n",
       "  4243,\n",
       "  3,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'id': '9976970',\n",
       " 'document': '아 더빙.. 진짜 짜증나네요 목소리',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1397fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가 지표 - accuracy 생성\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.argmax(axis=1)  # argmax \n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afa53b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_cosine_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# 옵티마이저와 스케줄러 설정\n",
    "learning_rate = 2e-5\n",
    "num_epochs = 3\n",
    "warmup_ratio = 0.1\n",
    "\n",
    "# 가중치 감쇠 설정\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06137b37",
   "metadata": {},
   "source": [
    "bias와 LayerNorm.weight에 감쇠를 적용하지 않는이유 (추측)\n",
    "- 감쇠를 적용하지 않아도 학습 안정성이 유지되고, 과적합에 덜 민감\n",
    "- 때문에 불필요한 연산 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "714519c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 옵티마이저 정의\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "\n",
    "# 학습 단계 계산\n",
    "t_total = len(train_dataset) // 16 * num_epochs  # 배치 크기 16 가정\n",
    "warmup_steps = int(t_total * warmup_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9313df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케줄러 정의\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c01c2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',             # 결과 저장 경로\n",
    "    num_train_epochs=num_epochs,        # 총 에폭 수\n",
    "    per_device_train_batch_size=16,     # 학습 배치 크기\n",
    "    per_device_eval_batch_size=16,      # 평가 배치 크기\n",
    "    learning_rate=learning_rate,        # 학습률\n",
    "    warmup_steps=warmup_steps,          # 워밍업 단계 수\n",
    "#     weight_decay=0.01,                  # 기본 weight decay\n",
    "    evaluation_strategy=\"epoch\",        # 에폭마다 평가\n",
    "    save_strategy=\"epoch\",              # 에폭마다 체크포인트 저장\n",
    "    logging_dir='./logs',               # 로그 저장 경로\n",
    "    logging_steps=500,                  # 로그 출력 빈도\n",
    "    report_to=\"none\",                   # 로깅 플랫폼 설정\n",
    "    save_total_limit=2,                 # 저장할 체크포인트 수 제한\n",
    "    load_best_model_at_end=True         # 최고의 모델 로드\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1160a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,                        # 학습할 모델\n",
    "    args=training_args,                 # TrainingArguments\n",
    "    train_dataset=train_dataset,        # 학습 데이터셋\n",
    "    eval_dataset=val_dataset,           # 검증 데이터셋\n",
    "    tokenizer=tokenizer,                # 토크나이저\n",
    "    compute_metrics=compute_metrics,    # 평가 지표 함수\n",
    "    optimizers=(optimizer, scheduler)   # 옵티마이저와 스케줄러 전달\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "be697e6d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28125' max='28125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28125/28125 3:05:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.273700</td>\n",
       "      <td>0.249071</td>\n",
       "      <td>0.900940</td>\n",
       "      <td>0.905243</td>\n",
       "      <td>0.897152</td>\n",
       "      <td>0.901179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.198400</td>\n",
       "      <td>0.268162</td>\n",
       "      <td>0.902740</td>\n",
       "      <td>0.884776</td>\n",
       "      <td>0.927621</td>\n",
       "      <td>0.905692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.377723</td>\n",
       "      <td>0.906360</td>\n",
       "      <td>0.900137</td>\n",
       "      <td>0.915584</td>\n",
       "      <td>0.907795</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-9375\n",
      "Configuration saved in ./results/checkpoint-9375/config.json\n",
      "Model weights saved in ./results/checkpoint-9375/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9375/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9375/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-18750\n",
      "Configuration saved in ./results/checkpoint-18750/config.json\n",
      "Model weights saved in ./results/checkpoint-18750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18750/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18750/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-28125\n",
      "Configuration saved in ./results/checkpoint-28125/config.json\n",
      "Model weights saved in ./results/checkpoint-28125/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-28125/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28125/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-18750] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-9375 (score: 0.24907077848911285).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28125, training_loss=0.21969765818277995, metrics={'train_runtime': 11154.1173, 'train_samples_per_second': 40.344, 'train_steps_per_second': 2.521, 'total_flos': 2.9599993728e+16, 'train_loss': 0.21969765818277995, 'epoch': 3.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cfb4c8f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 06:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.24907077848911285,\n",
       " 'eval_accuracy': 0.90094,\n",
       " 'eval_precision': 0.9052429052429053,\n",
       " 'eval_recall': 0.8971517101656536,\n",
       " 'eval_f1': 0.9011791464655534,\n",
       " 'eval_runtime': 380.371,\n",
       " 'eval_samples_per_second': 131.451,\n",
       " 'eval_steps_per_second': 8.216,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5573e5b",
   "metadata": {},
   "source": [
    "루브릭 달성!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1ba84",
   "metadata": {},
   "source": [
    "### Bucketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0712d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d96f390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # acc 함수\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a158aa4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 150000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 28125\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28125' max='28125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [28125/28125 3:10:28, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.252755</td>\n",
       "      <td>0.899860</td>\n",
       "      <td>0.899833</td>\n",
       "      <td>0.901442</td>\n",
       "      <td>0.900637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.199400</td>\n",
       "      <td>0.274378</td>\n",
       "      <td>0.900940</td>\n",
       "      <td>0.882578</td>\n",
       "      <td>0.926509</td>\n",
       "      <td>0.904010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.123400</td>\n",
       "      <td>0.377874</td>\n",
       "      <td>0.904640</td>\n",
       "      <td>0.900082</td>\n",
       "      <td>0.911810</td>\n",
       "      <td>0.905908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-9375\n",
      "Configuration saved in ./results/checkpoint-9375/config.json\n",
      "Model weights saved in ./results/checkpoint-9375/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-9375/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-9375/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-18750\n",
      "Configuration saved in ./results/checkpoint-18750/config.json\n",
      "Model weights saved in ./results/checkpoint-18750/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-18750/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-18750/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-28125] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ./results/checkpoint-28125\n",
      "Configuration saved in ./results/checkpoint-28125/config.json\n",
      "Model weights saved in ./results/checkpoint-28125/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-28125/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-28125/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-18750] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-9375 (score: 0.2527545094490051).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=28125, training_loss=0.2228409088812934, metrics={'train_runtime': 11430.0471, 'train_samples_per_second': 39.37, 'train_steps_per_second': 2.461, 'total_flos': 2.9599993728e+16, 'train_loss': 0.2228409088812934, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79035b6",
   "metadata": {},
   "source": [
    "### 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475a415f",
   "metadata": {},
   "source": [
    "뭔가 이번주는 몸도 아프고 이사준비도 해야해서 정신이없다..\\\n",
    "아직 못 다한 노드를 천천히 마무리 해야겠다.\\\n",
    "이번 노드에서는 베이스 실험들을 동기들 먼저 해주어서 성능을 높이는 방안에대해서만 생각할 수 있었다.\n",
    "- 해당 task에서 대부분의 모델들의 성능이 89% 정확도를 보여주었지만 skt에서 학습한 kobert 모델을 90%을 넘겨서 해당 방법을 착안하였다.\n",
    "    - classifier head에 dropout 적용 (50%)\n",
    "    - weight decay 설정 (bias, layerNorm 제외)\n",
    "    \n",
    "또한 실험 결과에 볼 수 있듯 해당 방법으로 정확도 90%을 넘겼고 이후 동적 패딩을 data collator를 사용하여 비교실험 하였다.\\\n",
    "실험 결과 큰 차이가 없는것으로 보아 지금 단계의 결과에서는 큰 성능 향상이 어려운것 같다.(학습시간도 비슷하다?)\\\n",
    "동적 패딩은 알고는 있었지만 실제 코드 구현이 이렇게 간편할 줄은 몰랐는데 앞으로 훈련할때 자주 애용해야겠다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe481925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
